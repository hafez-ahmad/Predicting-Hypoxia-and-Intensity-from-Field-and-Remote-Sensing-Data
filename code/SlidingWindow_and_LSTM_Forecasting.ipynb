{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d7d4da2",
   "metadata": {},
   "source": [
    "# Hypoxia Forecasting: Sliding-Window Classification (t+1, t+7, t+10) + LSTM\n",
    "\n",
    "This notebook builds supervised sliding-window classifiers to forecast hypoxia at multiple lead times and includes an LSTM sequence model. Horizons:\n",
    "- 1-day (t+1): uses t-1, t-2 (and differences)\n",
    "- 7-day (t+7): uses t-1..t-7 (or weekly aggregates)\n",
    "- 10-day (t+10): uses t-1..t-10 (or 2-week window)\n",
    "\n",
    "Baselines: persistence and monthly climatology. Models: RandomForest (default) with optional LightGBM/XGBoost if available. We use temporal cross-validation to avoid leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37de4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports & Configuration\n",
    "import os, warnings, math, json, glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from datetime import timedelta\n",
    "from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, balanced_accuracy_score, precision_score, recall_score,\n",
    "    f1_score, roc_auc_score, roc_curve, precision_recall_curve, brier_score_loss\n",
    ")\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "\n",
    "# Paths (edit these as needed)\n",
    "DATA_CSV = r\"C:\\\\Users\\\\hafez\\\\MSU\\\\Research\\\\msGOM\\\\mssound\\\\bloom\\\\data\\\\processed\\\\hypoxia_timeseries.csv\"\n",
    "MODELS_DIR = r\"C:\\\\Users\\\\hafez\\\\MSU\\\\Research\\\\msGOM\\\\mssound\\\\bloom\\\\data\\\\models_forecast\"\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "\n",
    "FEATURE_COLS = [\n",
    "    'chlor_a','nflh','poc','sst','Rrs_412','Rrs_443','Rrs_469','Rrs_488',\n",
    "    'Rrs_531','Rrs_547','Rrs_555','Rrs_645','Rrs_667','Rrs_678'\n",
    "]\n",
    "GROUP_COLS = ['lat','lon']  # per-pixel sequences\n",
    "TARGET_COL = 'label'  # 0/1 hypoxia indicator\n",
    "DATE_COL = 'date'\n",
    "\n",
    "# Helper: metrics dict\n",
    "\n",
    "def compute_metrics(y_true, y_prob, threshold=0.5):\n",
    "    y_pred = (y_prob >= threshold).astype(int)\n",
    "    return {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'balanced_accuracy': balanced_accuracy_score(y_true, y_pred),\n",
    "        'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "        'recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "        'f1': f1_score(y_true, y_pred, zero_division=0),\n",
    "        'roc_auc': roc_auc_score(y_true, y_prob) if len(np.unique(y_true))>1 else np.nan,\n",
    "        'brier': brier_score_loss(y_true, y_prob)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb24b256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Load and Inspect Time Series Data\n",
    "try:\n",
    "    df = pd.read_csv(DATA_CSV)\n",
    "    if DATE_COL in df.columns:\n",
    "        df[DATE_COL] = pd.to_datetime(df[DATE_COL])\n",
    "    df = df.sort_values([GROUP_COLS[0], GROUP_COLS[1], DATE_COL])\n",
    "    print('Loaded:', df.shape, 'time span:', df[DATE_COL].min(), '→', df[DATE_COL].max())\n",
    "    display(df.head())\n",
    "    print('\\nMissing values per column:')\n",
    "    print(df.isna().sum())\n",
    "except Exception as e:\n",
    "    print('Please set DATA_CSV to a valid CSV path:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9e8f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Preprocess and Engineer Features (differences, aggregates)\n",
    "# - Basic NA handling\n",
    "# - Optional scaling (per-feature global scaler)\n",
    "# - First differences and rolling stats per pixel\n",
    "\n",
    "# Simple NA fill: forward/backward per pixel, then global fill\n",
    "df[FEATURE_COLS] = df.groupby(GROUP_COLS)[FEATURE_COLS].apply(lambda g: g.ffill().bfill()).reset_index(level=GROUP_COLS, drop=True)\n",
    "df[FEATURE_COLS] = df[FEATURE_COLS].fillna(df[FEATURE_COLS].median())\n",
    "\n",
    "# Fit scaler on features only (global)\n",
    "scaler = MinMaxScaler()\n",
    "df[[f'{c}_scaled' for c in FEATURE_COLS]] = scaler.fit_transform(df[FEATURE_COLS])\n",
    "\n",
    "# Add first differences per pixel\n",
    "for c in FEATURE_COLS:\n",
    "    df[f'{c}_diff1'] = df.groupby(GROUP_COLS)[c].diff(1)\n",
    "    df[f'{c}_diff1'] = df[f'{c}_diff1'].fillna(0.0)\n",
    "\n",
    "# Optional rolling means (7-day) per pixel\n",
    "for c in FEATURE_COLS:\n",
    "    df[f'{c}_roll7'] = df.groupby(GROUP_COLS)[c].transform(lambda s: s.rolling(7, min_periods=1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044fcad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Create Sliding Windows for Multiple Horizons\n",
    "# Build (X, y) with window size k and horizon h per pixel, then stack across pixels.\n",
    "\n",
    "WINDOW_FEATURES = [\n",
    "    *FEATURE_COLS,\n",
    "    *[f'{c}_scaled' for c in FEATURE_COLS],\n",
    "    *[f'{c}_diff1' for c in FEATURE_COLS],\n",
    "]\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "def build_supervised(df_in: pd.DataFrame, k: int, h: int) -> Tuple[pd.DataFrame, pd.Series, pd.Series]:\n",
    "    \"\"\"Return X, y, and timestamps for samples using sliding window size k and forecast horizon h (in steps).\n",
    "    Assumes one row per (lat, lon, date).\"\"\"\n",
    "    frames = []\n",
    "    targets = []\n",
    "    times = []\n",
    "    for (lat, lon), g in df_in.groupby(GROUP_COLS):\n",
    "        g = g.sort_values(DATE_COL).reset_index(drop=True)\n",
    "        values = g[WINDOW_FEATURES].values\n",
    "        y_arr = g[TARGET_COL].values\n",
    "        for t in range(k, len(g) - h):\n",
    "            X_win = values[t-k:t].ravel(order='C')\n",
    "            y_t = y_arr[t + h]\n",
    "            frames.append(X_win)\n",
    "            targets.append(y_t)\n",
    "            times.append(g.loc[t + h, DATE_COL])\n",
    "    X = pd.DataFrame(frames)\n",
    "    y = pd.Series(targets)\n",
    "    ts = pd.Series(times)\n",
    "    return X, y, ts\n",
    "\n",
    "# Convenience: pick k based on horizon\n",
    "H_K = {1: 2, 7: 7, 10: 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbb751c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Train/Validation/Test Split by Time\n",
    "# We'll use the last 20% time for test, prior 20% for validation, rest for train.\n",
    "\n",
    "def temporal_split(ts: pd.Series, test_frac=0.2, val_frac=0.2):\n",
    "    ts_sorted = ts.sort_values().reset_index(drop=True)\n",
    "    n = len(ts_sorted)\n",
    "    t_test = ts_sorted.iloc[int((1 - test_frac) * n)]\n",
    "    t_val = ts_sorted.iloc[int((1 - test_frac - val_frac) * n)]\n",
    "    return t_val, t_test\n",
    "\n",
    "\n",
    "def split_by_time(X, y, ts, t_val, t_test):\n",
    "    train_idx = ts < t_val\n",
    "    val_idx = (ts >= t_val) & (ts < t_test)\n",
    "    test_idx = ts >= t_test\n",
    "    return (X[train_idx], y[train_idx]), (X[val_idx], y[val_idx]), (X[test_idx], y[test_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721ac965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5–7) Train Classifiers per Horizon (1, 7, 10 days)\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "RFC_PARAM_DIST = {\n",
    "    'n_estimators': randint(100, 400),\n",
    "    'max_depth': randint(3, 20),\n",
    "    'min_samples_split': randint(2, 10),\n",
    "    'min_samples_leaf': randint(1, 8),\n",
    "    'max_features': ['sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for h in [1, 7, 10]:\n",
    "    k = H_K[h]\n",
    "    print(f\"\\n=== Horizon h=+{h} days | window k={k} ===\")\n",
    "    X, y, ts = build_supervised(df, k=k, h=h)\n",
    "    t_val, t_test = temporal_split(ts, test_frac=0.2, val_frac=0.2)\n",
    "    (X_tr, y_tr), (X_va, y_va), (X_te, y_te) = split_by_time(X.values, y.values, ts, t_val, t_test)\n",
    "\n",
    "    # Model: RandomForest\n",
    "    base = RandomForestClassifier(class_weight='balanced', random_state=42, n_jobs=-1)\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=base,\n",
    "        param_distributions=RFC_PARAM_DIST,\n",
    "        n_iter=25,\n",
    "        scoring='f1',\n",
    "        cv=3,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    search.fit(X_tr, y_tr)\n",
    "    best = search.best_estimator_\n",
    "\n",
    "    # Validation evaluation\n",
    "    if hasattr(best, 'predict_proba'):\n",
    "        y_va_prob = best.predict_proba(X_va)[:, 1]\n",
    "        y_te_prob = best.predict_proba(X_te)[:, 1]\n",
    "    else:\n",
    "        # Fall back to decision function\n",
    "        y_va_raw = best.decision_function(X_va)\n",
    "        y_te_raw = best.decision_function(X_te)\n",
    "        y_va_prob = (y_va_raw - y_va_raw.min()) / (y_va_raw.ptp() + 1e-9)\n",
    "        y_te_prob = (y_te_raw - y_te_raw.min()) / (y_te_raw.ptp() + 1e-9)\n",
    "\n",
    "    m_va = compute_metrics(y_va, y_va_prob)\n",
    "    m_te = compute_metrics(y_te, y_te_prob)\n",
    "    print('Validation:', m_va)\n",
    "    print('Test:', m_te)\n",
    "\n",
    "    # Calibration\n",
    "    calibrator = CalibratedClassifierCV(best, method='sigmoid', cv=3)\n",
    "    calibrator.fit(X_va, y_va)\n",
    "    y_te_prob_cal = calibrator.predict_proba(X_te)[:, 1]\n",
    "    m_te_cal = compute_metrics(y_te, y_te_prob_cal)\n",
    "    print('Test (calibrated):', m_te_cal)\n",
    "\n",
    "    # Save models\n",
    "    joblib.dump(best, os.path.join(MODELS_DIR, f'best_model_t+{h}.pkl'))\n",
    "    joblib.dump(calibrator, os.path.join(MODELS_DIR, f'best_model_t+{h}_calibrated.pkl'))\n",
    "\n",
    "    # Curves\n",
    "    fpr, tpr, _ = roc_curve(y_te, y_te_prob_cal)\n",
    "    prec, rec, _ = precision_recall_curve(y_te, y_te_prob_cal)\n",
    "    fig, ax = plt.subplots(1,2, figsize=(10,4))\n",
    "    ax[0].plot(fpr, tpr, label=f'AUC={roc_auc_score(y_te, y_te_prob_cal):.3f}')\n",
    "    ax[0].plot([0,1],[0,1],'--',c='gray'); ax[0].set_title(f'ROC t+{h}')\n",
    "    ax[0].set_xlabel('FPR'); ax[0].set_ylabel('TPR'); ax[0].legend()\n",
    "    ax[1].plot(rec, prec); ax[1].set_title(f'PR t+{h}')\n",
    "    ax[1].set_xlabel('Recall'); ax[1].set_ylabel('Precision')\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "    results.append({'horizon': h, 'val': m_va, 'test': m_te, 'test_cal': m_te_cal})\n",
    "\n",
    "print('\\nSummary:')\n",
    "for r in results:\n",
    "    print(r['horizon'], r['test_cal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76039a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) LSTM Sequence Model for t+H Classification\n",
    "# Requires TensorFlow/Keras. If not installed, run:\n",
    "# pip install tensorflow==2.15.0\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers\n",
    "    TF_AVAILABLE = True\n",
    "    print('TensorFlow:', tf.__version__)\n",
    "except Exception as e:\n",
    "    TF_AVAILABLE = False\n",
    "    print('TensorFlow not available. Install tensorflow to run LSTM section.')\n",
    "\n",
    "\n",
    "def build_sequences(df_in: pd.DataFrame, k: int, h: int):\n",
    "    \"\"\"Return 3D arrays X_seq (N, k, F) and y (N,) aggregated across pixels.\"\"\"\n",
    "    feats = WINDOW_FEATURES  # could reduce for LSTM if needed\n",
    "    X_list, y_list = [], []\n",
    "    for _, g in df_in.groupby(GROUP_COLS):\n",
    "        g = g.sort_values(DATE_COL)\n",
    "        V = g[feats].values\n",
    "        y_arr = g[TARGET_COL].values\n",
    "        for t in range(k, len(g) - h):\n",
    "            X_list.append(V[t-k:t, :])\n",
    "            y_list.append(y_arr[t + h])\n",
    "    X_seq = np.stack(X_list, axis=0)\n",
    "    y = np.array(y_list).astype('int32')\n",
    "    return X_seq, y\n",
    "\n",
    "if TF_AVAILABLE:\n",
    "    for h in [1,7,10]:\n",
    "        k = H_K[h]\n",
    "        print(f\"\\n[LSTM] Horizon h=+{h} | window k={k}\")\n",
    "        X_seq, y = build_sequences(df, k=k, h=h)\n",
    "\n",
    "        # Temporal split by index\n",
    "        n = len(y)\n",
    "        i_val = int(n*0.6)\n",
    "        i_test = int(n*0.8)\n",
    "        X_tr, y_tr = X_seq[:i_val], y[:i_val]\n",
    "        X_va, y_va = X_seq[i_val:i_test], y[i_val:i_test]\n",
    "        X_te, y_te = X_seq[i_test:], y[i_test:]\n",
    "\n",
    "        # Model\n",
    "        model = keras.Sequential([\n",
    "            layers.Input(shape=(k, X_seq.shape[-1])),\n",
    "            layers.Masking(mask_value=0.0),\n",
    "            layers.LSTM(64, return_sequences=False),\n",
    "            layers.Dropout(0.2),\n",
    "            layers.Dense(32, activation='relu'),\n",
    "            layers.Dropout(0.2),\n",
    "            layers.Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        model.compile(optimizer=keras.optimizers.Adam(1e-3),\n",
    "                      loss='binary_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "        cb = [keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)]\n",
    "        hist = model.fit(X_tr, y_tr, validation_data=(X_va, y_va), epochs=30, batch_size=256, callbacks=cb, verbose=1)\n",
    "\n",
    "        y_te_prob = model.predict(X_te, verbose=0).ravel()\n",
    "        m_te = compute_metrics(y_te, y_te_prob)\n",
    "        print('[LSTM] Test:', m_te)\n",
    "\n",
    "        # Save\n",
    "        out_path = os.path.join(MODELS_DIR, f'lstm_t+{h}.keras')\n",
    "        model.save(out_path)\n",
    "        print('Saved:', out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28820b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9) Compare Lead-Time Performance & Plot\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "print('\\nTree-based results (per horizon, calibrated test):')\n",
    "pp.pprint(results)\n",
    "\n",
    "# 10) Notes & Next Steps (markdown below)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43028ff2",
   "metadata": {},
   "source": [
    "## Notes and Next Steps\n",
    "- Start with tree ensembles + lag features; move to Temporal CNN/LSTM if 7–10 day performance lags.\n",
    "- Add exogenous drivers: river discharge, wind stress, MLD, precipitation; align to timestamps.\n",
    "- Use blocked/rolling-origin CV; never shuffle across time.\n",
    "- Always compare to persistence and climatology baselines.\n",
    "- Calibrate per-horizon probabilities (Platt scaling) and verify with Brier score + reliability diagrams.\n",
    "- For raster inference, generate horizon-specific rasters using the same window builder and models saved as `best_model_t+H.pkl` or `lstm_t+H.keras`."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
